{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF2pZ8Wl35_o"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MyrVvRMYcwag"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFKC, Sequence\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "class BPE_token(object):\n",
        "    def __init__(self):\n",
        "        self.tokenizer = Tokenizer(BPE())\n",
        "        self.tokenizer.normalizer = Sequence([\n",
        "            NFKC()\n",
        "        ])\n",
        "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
        "        self.tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "    def bpe_train(self, paths):\n",
        "        trainer = BpeTrainer(vocab_size=50000, show_progress=True, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
        "            \"<s>\",\n",
        "            \"<pad>\",\n",
        "            \"</s>\",\n",
        "            \"<unk>\",\n",
        "            \"<mask>\"\n",
        "        ])\n",
        "        self.tokenizer.train(trainer, paths)\n",
        "\n",
        "    def save_tokenizer(self, location, prefix=None):\n",
        "        if not os.path.exists(location):\n",
        "            os.makedirs(location)\n",
        "        self.tokenizer.model.save(location, prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "91Rb230uddHJ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "# the folder 'text' contains all the files\n",
        "paths = [str(x) for x in Path(\"./dataset/\").glob(\"**/*.txt\")]\n",
        "tokenizer = BPE_token()\n",
        "# train the tokenizer model\n",
        "tokenizer.bpe_train(paths)\n",
        "# saving the tokenized data in our specified folder \n",
        "save_path = 'tokenized_data'\n",
        "tokenizer.save_tokenizer(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpk0nADwa6o1"
      },
      "source": [
        "#Model initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWsgtAcAa8Qm"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_99844\\824603770.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGPT2Config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTFGPT2LMHeadModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# loading tokenizer from the saved model path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\allis\\anaconda3\\envs\\TweetAI\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\allis\\anaconda3\\envs\\TweetAI\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\allis\\anaconda3\\envs\\TweetAI\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m   \u001b[1;31m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m   \u001b[1;31m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;31m# Externally in opensource we must enable exceptions to load the shared object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\allis\\anaconda3\\envs\\TweetAI\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\allis\\anaconda3\\envs\\TweetAI\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\allis\\anaconda3\\envs\\TweetAI\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\allis\\anaconda3\\envs\\TweetAI\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\allis\\anaconda3\\envs\\TweetAI\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mcreate_module\u001b[1;34m(self, spec)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import torch.optim.lr_scheduler\n",
        "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "# loading tokenizer from the saved model path\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
        "tokenizer.add_special_tokens({\n",
        "  \"eos_token\": \"</s>\",\n",
        "  \"bos_token\": \"<s>\",\n",
        "  \"unk_token\": \"<unk>\",\n",
        "  \"pad_token\": \"<pad>\",\n",
        "  \"mask_token\": \"<mask>\"\n",
        "})\n",
        "# creating the configurations from which the model can be made\n",
        "config = GPT2Config(\n",
        "  vocab_size=tokenizer.vocab_size,\n",
        "  bos_token_id=tokenizer.bos_token_id,\n",
        "  eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "# creating the model\n",
        "model = TFGPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBDKz8tucqmm"
      },
      "outputs": [],
      "source": [
        "single_string = ''\n",
        "for filename in paths:\n",
        "  with open(filename, \"r\", encoding='utf-8') as f:\n",
        "   x = f.read()\n",
        "  single_string += x + tokenizer.eos_token\n",
        "string_tokenized = tokenizer.encode(single_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIIULrPYctgH"
      },
      "outputs": [],
      "source": [
        "examples = []\n",
        "block_size = 100\n",
        "BATCH_SIZE = 12\n",
        "BUFFER_SIZE = 1000\n",
        "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
        "  examples.append(string_tokenized[i:i + block_size])\n",
        "inputs, labels = [], []\n",
        "for ex in examples:\n",
        "  inputs.append(ex[:-1])\n",
        "  labels.append(ex[1:])\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcZ0tbRhcvlJ"
      },
      "outputs": [],
      "source": [
        "# defining our optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "# definining our loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# defining our metric which we want to observe\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "# compiling the model\n",
        "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "HFl2c4dgcxWq",
        "outputId": "4013bfb5-1d05-4bd9-c4a8-b009c53bbe6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "75/75 [==============================] - 64s 596ms/step - loss: 7.5546 - output_1_loss: 7.5546 - output_1_accuracy: 0.1144 - output_2_1_accuracy: 0.0023 - output_2_2_accuracy: 0.0017 - output_2_3_accuracy: 0.0032 - output_2_4_accuracy: 0.0021 - output_2_5_accuracy: 0.0011 - output_2_6_accuracy: 8.8711e-04 - output_2_7_accuracy: 0.0033 - output_2_8_accuracy: 0.0015 - output_2_9_accuracy: 0.0029 - output_2_10_accuracy: 0.0019 - output_2_11_accuracy: 0.0037 - output_2_12_accuracy: 0.0015\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 46s 608ms/step - loss: 6.1059 - output_1_loss: 6.1059 - output_1_accuracy: 0.1835 - output_2_1_accuracy: 0.0023 - output_2_2_accuracy: 0.0018 - output_2_3_accuracy: 0.0030 - output_2_4_accuracy: 0.0024 - output_2_5_accuracy: 0.0013 - output_2_6_accuracy: 0.0014 - output_2_7_accuracy: 0.0023 - output_2_8_accuracy: 0.0017 - output_2_9_accuracy: 0.0031 - output_2_10_accuracy: 0.0028 - output_2_11_accuracy: 0.0030 - output_2_12_accuracy: 0.0015\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 46s 612ms/step - loss: 5.6217 - output_1_loss: 5.6217 - output_1_accuracy: 0.2101 - output_2_1_accuracy: 0.0022 - output_2_2_accuracy: 0.0019 - output_2_3_accuracy: 0.0030 - output_2_4_accuracy: 0.0024 - output_2_5_accuracy: 0.0018 - output_2_6_accuracy: 0.0019 - output_2_7_accuracy: 0.0020 - output_2_8_accuracy: 0.0019 - output_2_9_accuracy: 0.0024 - output_2_10_accuracy: 0.0024 - output_2_11_accuracy: 0.0029 - output_2_12_accuracy: 0.0018\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 46s 615ms/step - loss: 5.3776 - output_1_loss: 5.3776 - output_1_accuracy: 0.2306 - output_2_1_accuracy: 0.0023 - output_2_2_accuracy: 0.0020 - output_2_3_accuracy: 0.0031 - output_2_4_accuracy: 0.0022 - output_2_5_accuracy: 0.0020 - output_2_6_accuracy: 0.0024 - output_2_7_accuracy: 0.0021 - output_2_8_accuracy: 0.0019 - output_2_9_accuracy: 0.0022 - output_2_10_accuracy: 0.0027 - output_2_11_accuracy: 0.0027 - output_2_12_accuracy: 0.0019\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 47s 630ms/step - loss: 5.1875 - output_1_loss: 5.1875 - output_1_accuracy: 0.2508 - output_2_1_accuracy: 0.0022 - output_2_2_accuracy: 0.0021 - output_2_3_accuracy: 0.0029 - output_2_4_accuracy: 0.0022 - output_2_5_accuracy: 0.0021 - output_2_6_accuracy: 0.0024 - output_2_7_accuracy: 0.0020 - output_2_8_accuracy: 0.0019 - output_2_9_accuracy: 0.0022 - output_2_10_accuracy: 0.0030 - output_2_11_accuracy: 0.0027 - output_2_12_accuracy: 0.0021\n"
          ]
        },
        {
          "ename": "NotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a56ad1a5ca33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpoint_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/check/chec2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /content/check/chec2"
          ]
        }
      ],
      "source": [
        "num_epoch = 5\n",
        "\n",
        "checkpoint_filepath = '/content/check/chec2'\n",
        "history = model.fit(dataset, epochs=num_epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chHKG6_wnfln",
        "outputId": "65660b41-ef44-434b-b64e-a3a4f7749264"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Necesito 3 cosas de la gente. \"\n",
            "\"Conferencia de prensa matutina, desde Palacio Nacional. Conferencia de México. \n",
            "\n",
            "En la Ciudad de los mexicanos, a la corrupción, en el pueblo.\n",
            "\n",
            "El pueblo y a los pueblos del pueblo, el derecho a las pueblos y la salud de todos los más y en la construcción de las país, pero es el país. Es la transformación, no es la democracia, la justicia, que se lo que no se pueblo de nuestro país y el presidente de manera. Les en lasCOVID19. En el compromiso de nuestros pueblos, con la prensa en vivo. Se la vida. Hoy los los que el bienestar. El pueblo del presidente del Bienestar, como la Conferencia matutina. La pueblo es un pueblo en México, es los es una prensa, lo\n",
            "Les comparto el desarrollo del gobierno, Sonora. No en losCOVID que lo más la la política y los trabajadores de Estados Nacional, presidente pueblos de #COVID, nos es que la República, seCOVID\n",
            "La salud, los derechos con el gobierno es las México y no no lo los la Guardia Nacional y con los pueblo para el Conferencia en un la paz, al pueblo por la los\n"
          ]
        }
      ],
      "source": [
        "text = \"Necesito 3 cosas\"\n",
        "# encoding the input text\n",
        "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
        "# getting out output\n",
        "beam_output = model.generate(\n",
        "  input_ids,\n",
        "  max_length = 240,\n",
        "  num_beams = 5,\n",
        "  temperature = 0.7,\n",
        "  no_repeat_ngram_size=2,\n",
        "  num_return_sequences=5\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(beam_output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJF_S1SRHlzJ",
        "outputId": "6289d58d-6289-4ed1-f773-27a0e02f8b1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/check/tokenizer_config.json',\n",
              " '/content/check/special_tokens_map.json',\n",
              " '/content/check/vocab.json',\n",
              " '/content/check/merges.txt',\n",
              " '/content/check/added_tokens.json')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "import os\n",
        "output_dir = '/content/check'\n",
        "# creating directory if it is not present\n",
        "if not os.path.exists(output_dir):\n",
        "  os.mkdir(output_dir)\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "# save model and model configs\n",
        "model.save_pretrained(output_dir)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "# save tokenizer\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohRt-BZjJHpU"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
        "model  =TFGPT2LMHeadModel.from_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epsWN6D_1eUw"
      },
      "outputs": [],
      "source": [
        "!pip --version"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('TweetAI')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "eb525b03452340cd580e58f1dbbfd3df732d9c9d7a5558f64895b69254f71306"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
